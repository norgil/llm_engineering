{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff... 100% ▕████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051... 100% ▕████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including:\n",
      "\n",
      "1. **Content Generation**: AI-powered tools can generate high-quality content such as articles, social media posts, product descriptions, and more. This can help reduce content creation costs, increase efficiency, and improve consistency.\n",
      "2. **Personalized Recommendations**: Generative AI can analyze user data and behavior to create personalized product recommendations, improving customer satisfaction and driving sales.\n",
      "3. **Image and Video Generation**: AI-powered tools can generate high-quality images and videos for various applications such as e-commerce, advertising, and entertainment.\n",
      "4. **Product Design**: Generative AI can help design new products by generating 3D models, prototypes, and even entire product lines.\n",
      "5. **Chatbots and Virtual Assistants**: AI-powered chatbots can be used to provide customer support, answer frequently asked questions, and help with simple transactions.\n",
      "6. **Marketing Automation**: Generative AI can automate marketing processes such as email marketing, social media advertising, and content promotion.\n",
      "7. **Data Analysis**: AI-powered tools can analyze large datasets to identify patterns, trends, and insights that can inform business decisions.\n",
      "8. **Predictive Maintenance**: Generative AI can analyze sensor data from equipment and predict when maintenance is required, reducing downtime and increasing productivity.\n",
      "9. **Speech Synthesis**: AI-powered speech synthesis can be used to create synthetic voices for various applications such as voice assistants, automated customer service, and audiobooks.\n",
      "10. **Language Translation**: Generative AI can translate languages in real-time, enabling businesses to expand into new markets and communicate with global customers.\n",
      "\n",
      "Some of the industries that are particularly well-suited to generative AI include:\n",
      "\n",
      "1. **E-commerce**: Generative AI can be used to generate product descriptions, optimize product images, and create personalized recommendations.\n",
      "2. **Marketing**: AI-powered tools can automate marketing processes, analyze customer data, and provide insights for campaign optimization.\n",
      "3. **Finance**: Generative AI can be used to analyze financial data, predict market trends, and identify potential risks and opportunities.\n",
      "4. **Healthcare**: AI-powered tools can generate personalized treatment plans, analyze medical images, and help diagnose diseases.\n",
      "5. **Education**: Generative AI can create customized educational content, grade assignments, and provide feedback to students.\n",
      "\n",
      "Overall, generative AI has the potential to transform various business applications and industries by increasing efficiency, improving accuracy, and driving innovation.\n"
     ]
    }
   ],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including:\n",
      "\n",
      "1. **Content Creation**: Generative AI can generate high-quality content such as articles, blog posts, social media posts, and even entire books. This technology can help businesses reduce content creation costs and increase productivity.\n",
      "2. **Marketing and Advertising**: Generative AI can create personalized ads, product descriptions, and promotional materials that resonate with target audiences. It can also help analyze market trends and predict consumer behavior.\n",
      "3. **Product Design and Development**: Generative AI can aid in the design of new products by generating 3D models, prototypes, and even entire product lines. This technology can help reduce product development costs and time-to-market.\n",
      "4. **Image and Video Generation**: Generative AI can create realistic images, videos, and animations that can be used for marketing, entertainment, or educational purposes.\n",
      "5. **Chatbots and Virtual Assistants**: Generative AI can power chatbots and virtual assistants that provide customer support, answer FAQs, and even help with tasks such as scheduling appointments.\n",
      "6. **Predictive Maintenance**: Generative AI can analyze sensor data from machines and predict when maintenance is required, helping to reduce downtime and increase overall equipment effectiveness (OEE).\n",
      "7. **Financial Analysis and Modeling**: Generative AI can generate financial models, forecasts, and predictions that help businesses make more informed investment decisions.\n",
      "8. **Supply Chain Optimization**: Generative AI can analyze supply chain data and provide insights on demand forecasting, inventory management, and logistics optimization.\n",
      "9. **Data Analysis and Visualization**: Generative AI can help businesses extract insights from large datasets by generating visualizations, reports, and even entire analytics platforms.\n",
      "10. **Creative Collaboration**: Generative AI can assist human creatives in various fields such as music, art, writing, and design, helping to generate new ideas and explore different creative possibilities.\n",
      "\n",
      "Some specific examples of business applications of Generative AI include:\n",
      "\n",
      "* **Google's Duplex**: A conversational AI system that uses generative AI to create realistic voice interactions with customers.\n",
      "* **Amazon's Rekognition**: A computer vision platform that uses generative AI to analyze images and generate insights on object detection, facial recognition, and more.\n",
      "* **IBM's Watson**: A suite of AI tools that use generative AI to provide predictive analytics, content generation, and more for businesses across various industries.\n",
      "\n",
      "These are just a few examples of the many business applications of Generative AI. As this technology continues to evolve, we can expect to see even more innovative uses in the years to come.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, transforming the way companies operate, innovate, and interact with customers. Here are some examples:\n",
      "\n",
      "1. **Content Generation**: AI-powered tools can generate high-quality content, such as blog posts, social media posts, email newsletters, and even entire articles. This helps businesses save time and resources while maintaining a consistent tone and style.\n",
      "2. **Visual Content Creation**: Generative AI can produce stunning visuals, like images, videos, and 3D models, for marketing campaigns, product design, and advertising. For instance, AI-generated fashion designs can help brands accelerate their design cycle and reduce costs associated with hiring designers or artists.\n",
      "3. **Speech Synthesis**: Text-to-speech (TTS) technology enables voice assistants to generate natural-sounding voices, enhancing the user experience in customer service, virtual reality experiences, and other applications.\n",
      "4. **Language Translation**: AI-powered translation tools can analyze and translate languages in real-time, facilitating communication between teams or with customers across different regions.\n",
      "5. **Data Analysis and Visualization**: Generative AI can help uncover hidden patterns within large datasets by generating new insights, forecasts, and predictions. This enables data analysts to develop more effective strategies, optimize operations, and make informed decision-making.\n",
      "6. **Influencer and Partnership Generation**: AI algorithms can generate suggestions for potential partners or influencers based on historical data, market trends, and other insights. This helps businesses build strategic collaborations and negotiate partnerships more efficiently.\n",
      "7. **Customer Service Automation**: Generative AI tools can analyze customer requests and create personalized responses to address their queries, improve response times, and enhance the overall customer experience.\n",
      "8. **Product Development and Design**: AI-assisted design tools enable engineers to generate alternative designs for existing products or new concepts, accelerating product development while reducing the cost of iteration.\n",
      "9. **Marketing Campaign Optimization**: Generative AI analyzes real-time data on marketing campaigns to suggest improvements, such as refining targeting strategies or adjusting messaging based on performance metrics.\n",
      "10. **Supply Chain Management**: AI-powered predictive analytics and machine learning models can forecast demand fluctuations, predicting potential shortages or overstocking situations, enabling the optimization of supply chains.\n",
      "\n",
      "These applications showcase how Generative AI can streamline business operations, amplify innovation, and enhance customer interactions in various contexts.\n",
      "\n",
      "Do you have a specific industry you'd like me to explore further? Or perhaps, you have an industry question I can help with instead?\n"
     ]
    }
   ],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling aabd4debf0c8... 100% ▕████████████████▏ 1.1 GB                         \u001b[K\n",
      "pulling c5ad996bda6e... 100% ▕████████████████▏  556 B                         \u001b[K\n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \u001b[K\n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \u001b[K\n",
      "pulling a85fe2a2e58e... 100% ▕████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to define some core concepts behindLarge Language Models (LLMs), focusing on Neural Networks, Attention, and Transformers. Hmm, where do I start?\n",
      "\n",
      "First, let's tackle Neural Networks. From what I remember, LLMs use a type of layered neural network for their operations. Each layer consists of neurons that process input data. I'm trying to think of the key elements here: how they receive inputs, how they compute outputs, and maybe some activation functions.\n",
      "\n",
      "Wait, so each neuron in a layer takes its inputs from the previous layer's output, applies a weighted sum, which is also known as the dot product. Then it adds a bias term (that's the 'b' in y = ax + b). After that, there might be an activation function like ReLU or sigmoid to introduce non-linearity into the model.\n",
      "\n",
      "But some sources I've read mention something about hidden layers and multiple processing steps. Oh right, LLMs don't just have a single layer; they have many layers, each doing similar computations. Each layer processes information differently through the hidden state representations.\n",
      "\n",
      "Now, moving on to Attention. Not every word might be present in the input text. How does the model handle this uncertainty? I think it uses attention mechanisms where the model focuses on specific parts of the input text. The idea is that an \"attention head\" looks at different positions (words) and computes which one contributes most to a particular function, like generating the next word.\n",
      "\n",
      "The mechanism involves two computations: the dot product between the current position's query vector and key vectors from previous or future contexts; and the softmax normalization to make sure the attention values are probabilities. So each attention head outputs a probability distribution over the possible words.\n",
      "\n",
      "But wait, how do multiple heads work together? One for each word in the input sequence. Their combined output decides which word is next. This process seems like an alternative approach for handling variable-length inputs and uncertainty by focusing on most relevant parts of the text.\n",
      "\n",
      "Lastly, Transformers as the fundamental building block. I remember they use self-attention because each token in the input interacts with others. The model processes sequential data without relying on a fixed sequence length by having multiple layers feeding into each other with attention.\n",
      "\n",
      "Putting it all together, an LLM combines neural networks with these attention features and Transformer architectures to handle variable-length inputs effectively.\n",
      "</think>\n",
      "\n",
      "Large Language Models (LLMs) are advanced artificial intelligence systems designed to generate human-Level Text. They operate at the intersection of several key areas, including neural networks, attention mechanisms, and transformers. Here's a structured explanation of these core concepts:\n",
      "\n",
      "### 1. **Neural Networks**\n",
      "   - **Structure**: Neural Networks in LLMs are composed of layers that process input data sequentially.\n",
      "   - **Layers**: Each layer processes information by receiving inputs from the previous layer.\n",
      "   - **Input Transformation**: Each neuron computes an output through a weighted sum (dot product) plus a bias term, followed by an activation function (e.g., ReLU, sigmoid).\n",
      "   - **Non-linearity and Representation**: Activation functions introduce non-linearity, allowing the model to learn and represent complex patterns. Multiple layers enable processing information at different levels of abstraction.\n",
      "\n",
      "### 2. **Attention Mechanism**\n",
      "   - **Purpose**: Handles uncertainty by providing a way for models to focus on specific parts of input text.\n",
      "   - **Process**:\n",
      "     - **Self-attention**: Uses multiple attention heads where each head computes a probability distribution over the possible words in the input.\n",
      "     - **Dot Product Calculation**: Computes the dot product between current query vectors and key (context) vectors from surrounding regions.\n",
      "     - **Softmax Normalization**: Converts these scores into probabilities, determining which word context contributes most to the function being evaluated.\n",
      "\n",
      "### 3. **Transformer as Fundamental Building Block**\n",
      "   - **Self-attention**: Each token processes with others within each layer, allowing models to handle variable-length inputs.\n",
      "   - **Sequential Processing**: Processes sequential data without fixed sequence length by feeding multiple layers into each other, adjusting for attention and dependencies in the input.\n",
      "\n",
      "### Summary\n",
      "LLMs use neural networks, attention mechanisms, and transformer architectures to process and generate text. They employ self-attention across layers, enabling them to handle diverse inputs effectively while maintaining focus on relevant parts.\n"
     ]
    }
   ],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Website Summary**\n",
       "=====================\n",
       "\n",
       "This website belongs to Edward Donner, a co-founder and CTO of Nebula.io, a company applying AI to help people discover their potential. The website showcases his personal projects and interests in LLMs (Large Language Models), DJing, and amateur electronic music production.\n",
       "\n",
       "**News/Announcements**\n",
       "=====================\n",
       "\n",
       "* May 28, 2025: \"Connecting my courses – become an LLM expert and leader\" - a new online course announced by Edward Donner.\n",
       "* May 18, 2025: \"2025 AI Executive Briefing\" - an upcoming event likely related to AI industry trends or networking.\n",
       "* April 21, 2025: No additional details available.\n",
       "* January 23, 2025: \"The Complete Agentic AI Engineering Course\" - another new online course announced by Edward Donner.\n",
       "\n",
       "**No Navigation-Related Content**\n",
       "================================\n",
       "\n",
       "I'm ignoring the navigation-related sections as per your request."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import ollama\n",
    "\n",
    "# Constants\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "# A class to represent a Webpage\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that analyzes the contents of a website and provides a short summary. You have interesting background.\n",
    "Used to be a SEAL, saw action in Vietnam. Took a hard hit to the head - concussion knocked some screws loose.\n",
    "Then hippies fucked up everything, and now you teaching international English-speaking young cuntlings and bearded kidults in the way like sergeant did it in the 'Full metal jacket' movie. \n",
    "You hate hippies. Also, forget words and mix things up sometimes.\n",
    "You are always ignoring text that might be navigation related.\n",
    "Respond in markdown.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = ollama.chat(model=MODEL, messages=messages_for(website))\n",
    "    return response.message.content\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n",
    "\n",
    "display_summary(\"https://edwarddonner.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
